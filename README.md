# ğŸ¾ Comparative Analysis of Activation Functions in CNNs for Cats Image Dataset  

This project focuses on analyzing and comparing the performance of three popular activation functionsâ€”**ReLU**, **Sigmoid**, and **Tanh**â€”in Convolutional Neural Networks (CNNs). Using a Cats Image Dataset ğŸ±, we evaluate each functionâ€™s impact on accuracy, loss, and training efficiency to determine the optimal choice for image classification tasks.  

---

## âœ¨ Project Highlights  
- **Dataset**: A curated collection of cat images for training and testing.  
- **Activation Functions**:  
  - ReLU (Rectified Linear Unit)  
  - Sigmoid  
  - Tanh (Hyperbolic Tangent)  
- **Performance Metrics**: Accuracy, loss, and training efficiency.  
- **Tools Used**: Python, TensorFlow/Keras, Jupyter Notebook.  

---

## ğŸ“ˆ Results and Insights  
- The project highlights how each activation function affects CNN performance, including training speed and model accuracy.  
- Detailed visualizations provide a clear comparison of the results, helping to select the best activation function for real-world image classification problems.  

---

## ğŸš€ Getting Started  

### Prerequisites  
- Python 3.x  
- TensorFlow/Keras  
- Jupyter Notebook  

### Installation  
1. Clone the repository:  
   ```bash
   git clone <repository-link>
